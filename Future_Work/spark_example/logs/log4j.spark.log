20/11/16 14:26:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/11/16 14:26:41 INFO SecurityManager: Changing view acls to: grahamchickering
20/11/16 14:26:41 INFO SecurityManager: Changing modify acls to: grahamchickering
20/11/16 14:26:41 INFO SecurityManager: Changing view acls groups to: 
20/11/16 14:26:41 INFO SecurityManager: Changing modify acls groups to: 
20/11/16 14:26:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(grahamchickering); groups with view permissions: Set(); users  with modify permissions: Set(grahamchickering); groups with modify permissions: Set()
20/11/16 14:26:42 INFO HiveConf: Found configuration file file:/Users/grahamchickering/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/16 14:26:42 INFO SparkContext: Running Spark version 3.0.0
20/11/16 14:26:42 INFO ResourceUtils: ==============================================================
20/11/16 14:26:42 INFO ResourceUtils: Resources for spark.driver:

20/11/16 14:26:42 INFO ResourceUtils: ==============================================================
20/11/16 14:26:42 INFO SparkContext: Submitted application: sparklyr
20/11/16 14:26:42 INFO SecurityManager: Changing view acls to: grahamchickering
20/11/16 14:26:42 INFO SecurityManager: Changing modify acls to: grahamchickering
20/11/16 14:26:42 INFO SecurityManager: Changing view acls groups to: 
20/11/16 14:26:42 INFO SecurityManager: Changing modify acls groups to: 
20/11/16 14:26:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(grahamchickering); groups with view permissions: Set(); users  with modify permissions: Set(grahamchickering); groups with modify permissions: Set()
20/11/16 14:26:42 INFO Utils: Successfully started service 'sparkDriver' on port 54752.
20/11/16 14:26:42 INFO SparkEnv: Registering MapOutputTracker
20/11/16 14:26:42 INFO SparkEnv: Registering BlockManagerMaster
20/11/16 14:26:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/11/16 14:26:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/11/16 14:26:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/11/16 14:26:43 INFO DiskBlockManager: Created local directory at /private/var/folders/q3/tpdx666x4cv2zhyblrxyhyl00000gn/T/blockmgr-89363991-4994-49b6-8f10-edd6041192b1
20/11/16 14:26:43 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
20/11/16 14:26:43 INFO SparkEnv: Registering OutputCommitCoordinator
20/11/16 14:26:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/11/16 14:26:43 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://localhost:4040
20/11/16 14:26:43 INFO SparkContext: Added JAR file:/Library/Frameworks/R.framework/Versions/4.0/Resources/library/sparklyr/java/sparklyr-3.0-2.12.jar at spark://localhost:54752/jars/sparklyr-3.0-2.12.jar with timestamp 1605554803544
20/11/16 14:26:43 INFO Executor: Starting executor ID driver on host localhost
20/11/16 14:26:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54753.
20/11/16 14:26:43 INFO NettyBlockTransferService: Server created on localhost:54753
20/11/16 14:26:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/11/16 14:26:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 54753, None)
20/11/16 14:26:43 INFO BlockManagerMasterEndpoint: Registering block manager localhost:54753 with 912.3 MiB RAM, BlockManagerId(driver, localhost, 54753, None)
20/11/16 14:26:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 54753, None)
20/11/16 14:26:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 54753, None)
20/11/16 14:26:44 INFO SharedState: loading hive config file: file:/Users/grahamchickering/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/16 14:26:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/grahamchickering/Desktop/Stat%20495/STAT495F20-project-Chickering/spark_example/spark-warehouse').
20/11/16 14:26:44 INFO SharedState: Warehouse path is 'file:/Users/grahamchickering/Desktop/Stat%20495/STAT495F20-project-Chickering/spark_example/spark-warehouse'.
20/11/16 14:26:47 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.7 using Spark classes.
20/11/16 14:26:47 INFO HiveConf: Found configuration file file:/Users/grahamchickering/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/16 14:26:48 INFO SessionState: Created HDFS directory: /tmp/hive/grahamchickering
20/11/16 14:26:48 INFO SessionState: Created local directory: /var/folders/q3/tpdx666x4cv2zhyblrxyhyl00000gn/T/grahamchickering
20/11/16 14:26:48 INFO SessionState: Created HDFS directory: /tmp/hive/grahamchickering/2384e8bc-0875-4b43-a807-76de4f8a33d7
20/11/16 14:26:48 INFO SessionState: Created local directory: /var/folders/q3/tpdx666x4cv2zhyblrxyhyl00000gn/T/grahamchickering/2384e8bc-0875-4b43-a807-76de4f8a33d7
20/11/16 14:26:48 INFO SessionState: Created HDFS directory: /tmp/hive/grahamchickering/2384e8bc-0875-4b43-a807-76de4f8a33d7/_tmp_space.db
20/11/16 14:26:48 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is file:/Users/grahamchickering/Desktop/Stat%20495/STAT495F20-project-Chickering/spark_example/spark-warehouse
20/11/16 14:26:49 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
20/11/16 14:26:49 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
20/11/16 14:26:49 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
20/11/16 14:26:49 INFO ObjectStore: ObjectStore, initialize called
20/11/16 14:26:49 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
20/11/16 14:26:49 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
20/11/16 14:26:50 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/11/16 14:26:51 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/11/16 14:26:51 INFO ObjectStore: Initialized ObjectStore
20/11/16 14:26:51 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
20/11/16 14:26:51 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore grahamchickering@10.112.183.126
20/11/16 14:26:52 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/11/16 14:26:52 INFO HiveMetaStore: Added admin role in metastore
20/11/16 14:26:52 INFO HiveMetaStore: Added public role in metastore
20/11/16 14:26:52 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/11/16 14:26:52 INFO HiveMetaStore: 0: get_all_functions
20/11/16 14:26:52 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_all_functions	
20/11/16 14:26:52 INFO HiveMetaStore: 0: get_database: default
20/11/16 14:26:52 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: default	
20/11/16 14:26:52 INFO HiveMetaStore: 0: get_database: global_temp
20/11/16 14:26:52 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: global_temp	
20/11/16 14:26:52 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
20/11/16 14:26:52 INFO HiveMetaStore: 0: get_database: default
20/11/16 14:26:52 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: default	
20/11/16 14:26:52 INFO HiveMetaStore: 0: get_database: default
20/11/16 14:26:52 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: default	
20/11/16 14:26:52 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/16 14:26:52 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/16 14:26:53 INFO CodeGenerator: Code generated in 229.005621 ms
20/11/16 14:26:53 INFO CodeGenerator: Code generated in 10.161998 ms
20/11/16 14:26:53 INFO SparkContext: Starting job: count at utils.scala:114
20/11/16 14:26:53 INFO DAGScheduler: Registering RDD 2 (count at utils.scala:114) as input to shuffle 0
20/11/16 14:26:53 INFO DAGScheduler: Got job 0 (count at utils.scala:114) with 1 output partitions
20/11/16 14:26:53 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:114)
20/11/16 14:26:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/11/16 14:26:53 INFO DAGScheduler: Missing parents: List()
20/11/16 14:26:53 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114), which has no missing parents
20/11/16 14:26:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
20/11/16 14:26:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
20/11/16 14:26:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:54753 (size: 5.0 KiB, free: 912.3 MiB)
20/11/16 14:26:53 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1200
20/11/16 14:26:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/16 14:26:53 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/11/16 14:26:53 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
20/11/16 14:26:53 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)
20/11/16 14:26:53 INFO Executor: Fetching spark://localhost:54752/jars/sparklyr-3.0-2.12.jar with timestamp 1605554803544
20/11/16 14:26:53 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:54752 after 30 ms (0 ms spent in bootstraps)
20/11/16 14:26:53 INFO Utils: Fetching spark://localhost:54752/jars/sparklyr-3.0-2.12.jar to /private/var/folders/q3/tpdx666x4cv2zhyblrxyhyl00000gn/T/spark-66a6f45d-4fec-404e-a08b-f10d46b598db/userFiles-8d1cf6d0-e792-46c0-8544-4d218f9cbea3/fetchFileTemp4326155965827599649.tmp
20/11/16 14:26:53 INFO Executor: Adding file:/private/var/folders/q3/tpdx666x4cv2zhyblrxyhyl00000gn/T/spark-66a6f45d-4fec-404e-a08b-f10d46b598db/userFiles-8d1cf6d0-e792-46c0-8544-4d218f9cbea3/sparklyr-3.0-2.12.jar to class loader
20/11/16 14:26:54 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/11/16 14:26:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
20/11/16 14:26:54 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 2641 bytes result sent to driver
20/11/16 14:26:54 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 461 ms on localhost (executor driver) (1/1)
20/11/16 14:26:54 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/11/16 14:26:54 INFO DAGScheduler: ResultStage 1 (count at utils.scala:114) finished in 0.622 s
20/11/16 14:26:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/11/16 14:26:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
20/11/16 14:26:54 INFO DAGScheduler: Job 0 finished: count at utils.scala:114, took 0.667074 s
20/11/16 14:26:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:54753 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/11/16 14:27:28 INFO HiveMetaStore: 0: get_database: default
20/11/16 14:27:28 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: default	
20/11/16 14:27:28 INFO HiveMetaStore: 0: get_database: default
20/11/16 14:27:28 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: default	
20/11/16 14:27:28 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/16 14:27:28 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/16 14:27:29 INFO SparkContext: Starting job: collect at utils.scala:41
20/11/16 14:27:29 INFO DAGScheduler: Job 1 finished: collect at utils.scala:41, took 0.000149 s
20/11/16 16:52:02 INFO CodeGenerator: Code generated in 24.478194 ms
20/11/16 16:52:02 INFO CodeGenerator: Code generated in 9.004962 ms
20/11/16 16:52:02 INFO SparkContext: Starting job: collect at utils.scala:116
20/11/16 16:52:02 INFO DAGScheduler: Got job 2 (collect at utils.scala:116) with 1 output partitions
20/11/16 16:52:02 INFO DAGScheduler: Final stage: ResultStage 2 (collect at utils.scala:116)
20/11/16 16:52:02 INFO DAGScheduler: Parents of final stage: List()
20/11/16 16:52:02 INFO DAGScheduler: Missing parents: List()
20/11/16 16:52:02 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[23] at collect at utils.scala:116), which has no missing parents
20/11/16 16:52:02 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 50.5 KiB, free 912.3 MiB)
20/11/16 16:52:02 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 16.1 KiB, free 912.2 MiB)
20/11/16 16:52:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:54753 (size: 16.1 KiB, free: 912.3 MiB)
20/11/16 16:52:02 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
20/11/16 16:52:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[23] at collect at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/11/16 16:52:02 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
20/11/16 16:52:02 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7393 bytes)
20/11/16 16:52:02 INFO TaskSchedulerImpl: Registered BarrierCoordinator endpoint
20/11/16 16:52:02 INFO TaskSchedulerImpl: Successfully scheduled all the 1 tasks for barrier stage 2.
20/11/16 16:52:02 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
20/11/16 16:52:03 INFO CodeGenerator: Code generated in 10.628617 ms
20/11/16 16:52:03 INFO CodeGenerator: Code generated in 12.338138 ms
20/11/16 16:52:04 INFO CodeGenerator: Code generated in 18.124922 ms
20/11/16 16:52:04 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 1544 bytes result sent to driver
20/11/16 16:52:04 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 2013 ms on localhost (executor driver) (1/1)
20/11/16 16:52:04 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/11/16 16:52:04 INFO DAGScheduler: ResultStage 2 (collect at utils.scala:116) finished in 2.025 s
20/11/16 16:52:04 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
20/11/16 16:52:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
20/11/16 16:52:04 INFO DAGScheduler: Job 2 finished: collect at utils.scala:116, took 2.030632 s
20/11/16 16:52:04 INFO CodeGenerator: Code generated in 9.612846 ms
20/11/16 16:52:04 INFO CodeGenerator: Code generated in 10.580138 ms
20/11/16 16:52:04 INFO SparkContext: Starting job: count at utils.scala:114
20/11/16 16:52:04 INFO DAGScheduler: Registering RDD 25 (count at utils.scala:114) as input to shuffle 1
20/11/16 16:52:04 INFO DAGScheduler: Got job 3 (count at utils.scala:114) with 1 output partitions
20/11/16 16:52:04 INFO DAGScheduler: Final stage: ResultStage 4 (count at utils.scala:114)
20/11/16 16:52:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
20/11/16 16:52:05 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
20/11/16 16:52:05 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[25] at count at utils.scala:114), which has no missing parents
20/11/16 16:52:05 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 54.0 KiB, free 912.2 MiB)
20/11/16 16:52:05 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 17.8 KiB, free 912.2 MiB)
20/11/16 16:52:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:54753 (size: 17.8 KiB, free: 912.3 MiB)
20/11/16 16:52:05 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1200
20/11/16 16:52:05 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[25] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/16 16:52:05 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
20/11/16 16:52:05 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7382 bytes)
20/11/16 16:52:05 INFO TaskSchedulerImpl: Successfully scheduled all the 1 tasks for barrier stage 3.
20/11/16 16:52:05 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
20/11/16 16:52:06 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 2000 bytes result sent to driver
20/11/16 16:52:06 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 1798 ms on localhost (executor driver) (1/1)
20/11/16 16:52:06 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/11/16 16:52:06 INFO DAGScheduler: ShuffleMapStage 3 (count at utils.scala:114) finished in 1.839 s
20/11/16 16:52:06 INFO DAGScheduler: looking for newly runnable stages
20/11/16 16:52:06 INFO DAGScheduler: running: Set()
20/11/16 16:52:06 INFO DAGScheduler: waiting: Set(ResultStage 4)
20/11/16 16:52:06 INFO DAGScheduler: failed: Set()
20/11/16 16:52:06 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[28] at count at utils.scala:114), which has no missing parents
20/11/16 16:52:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.1 KiB, free 912.2 MiB)
20/11/16 16:52:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.2 MiB)
20/11/16 16:52:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:54753 (size: 5.0 KiB, free: 912.3 MiB)
20/11/16 16:52:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1200
20/11/16 16:52:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[28] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/16 16:52:06 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
20/11/16 16:52:06 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3, localhost, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
20/11/16 16:52:06 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
20/11/16 16:52:06 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/11/16 16:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
20/11/16 16:52:06 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 2648 bytes result sent to driver
20/11/16 16:52:06 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 29 ms on localhost (executor driver) (1/1)
20/11/16 16:52:06 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
20/11/16 16:52:06 INFO DAGScheduler: ResultStage 4 (count at utils.scala:114) finished in 0.037 s
20/11/16 16:52:06 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
20/11/16 16:52:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
20/11/16 16:52:06 INFO DAGScheduler: Job 3 finished: count at utils.scala:114, took 1.902667 s
20/11/16 16:56:44 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:54753 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/11/16 16:56:44 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:54753 in memory (size: 16.1 KiB, free: 912.3 MiB)
20/11/16 16:56:44 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:54753 in memory (size: 17.8 KiB, free: 912.3 MiB)
20/11/16 17:07:36 INFO SparkContext: Invoking stop() from shutdown hook
20/11/16 17:07:36 INFO SparkUI: Stopped Spark web UI at http://localhost:4040
20/11/16 17:07:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/11/16 17:07:36 INFO MemoryStore: MemoryStore cleared
20/11/16 17:07:36 INFO BlockManager: BlockManager stopped
20/11/16 17:07:36 INFO BlockManagerMaster: BlockManagerMaster stopped
20/11/16 17:07:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/11/16 17:07:36 INFO SparkContext: Successfully stopped SparkContext
20/11/16 17:07:36 INFO ShutdownHookManager: Shutdown hook called
20/11/16 17:07:36 INFO ShutdownHookManager: Deleting directory /private/var/folders/q3/tpdx666x4cv2zhyblrxyhyl00000gn/T/spark-66a6f45d-4fec-404e-a08b-f10d46b598db
20/11/16 17:07:36 INFO ShutdownHookManager: Deleting directory /private/var/folders/q3/tpdx666x4cv2zhyblrxyhyl00000gn/T/spark-a33638b7-3eca-459e-ac1c-f5d3121bace1
20/11/16 17:13:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/11/16 17:13:28 INFO SecurityManager: Changing view acls to: grahamchickering
20/11/16 17:13:28 INFO SecurityManager: Changing modify acls to: grahamchickering
20/11/16 17:13:28 INFO SecurityManager: Changing view acls groups to: 
20/11/16 17:13:28 INFO SecurityManager: Changing modify acls groups to: 
20/11/16 17:13:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(grahamchickering); groups with view permissions: Set(); users  with modify permissions: Set(grahamchickering); groups with modify permissions: Set()
20/11/16 17:13:29 INFO HiveConf: Found configuration file file:/Users/grahamchickering/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/16 17:13:29 INFO SparkContext: Running Spark version 3.0.0
20/11/16 17:13:29 INFO ResourceUtils: ==============================================================
20/11/16 17:13:29 INFO ResourceUtils: Resources for spark.driver:

20/11/16 17:13:29 INFO ResourceUtils: ==============================================================
20/11/16 17:13:29 INFO SparkContext: Submitted application: sparklyr
20/11/16 17:13:29 INFO SecurityManager: Changing view acls to: grahamchickering
20/11/16 17:13:29 INFO SecurityManager: Changing modify acls to: grahamchickering
20/11/16 17:13:29 INFO SecurityManager: Changing view acls groups to: 
20/11/16 17:13:29 INFO SecurityManager: Changing modify acls groups to: 
20/11/16 17:13:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(grahamchickering); groups with view permissions: Set(); users  with modify permissions: Set(grahamchickering); groups with modify permissions: Set()
20/11/16 17:13:29 INFO Utils: Successfully started service 'sparkDriver' on port 58470.
20/11/16 17:13:29 INFO SparkEnv: Registering MapOutputTracker
20/11/16 17:13:30 INFO SparkEnv: Registering BlockManagerMaster
20/11/16 17:13:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/11/16 17:13:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/11/16 17:13:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/11/16 17:13:30 INFO DiskBlockManager: Created local directory at /private/var/folders/q3/tpdx666x4cv2zhyblrxyhyl00000gn/T/blockmgr-79b00290-4abb-464b-a6aa-e3205c40029d
20/11/16 17:13:30 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
20/11/16 17:13:30 INFO SparkEnv: Registering OutputCommitCoordinator
20/11/16 17:13:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/11/16 17:13:30 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://localhost:4040
20/11/16 17:13:30 INFO SparkContext: Added JAR file:/Library/Frameworks/R.framework/Versions/4.0/Resources/library/sparklyr/java/sparklyr-3.0-2.12.jar at spark://localhost:58470/jars/sparklyr-3.0-2.12.jar with timestamp 1605564810529
20/11/16 17:13:30 INFO Executor: Starting executor ID driver on host localhost
20/11/16 17:13:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58472.
20/11/16 17:13:30 INFO NettyBlockTransferService: Server created on localhost:58472
20/11/16 17:13:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/11/16 17:13:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 58472, None)
20/11/16 17:13:30 INFO BlockManagerMasterEndpoint: Registering block manager localhost:58472 with 912.3 MiB RAM, BlockManagerId(driver, localhost, 58472, None)
20/11/16 17:13:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 58472, None)
20/11/16 17:13:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 58472, None)
20/11/16 17:13:31 INFO SharedState: loading hive config file: file:/Users/grahamchickering/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/16 17:13:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/grahamchickering/Desktop/Stat%20495/STAT495F20-project-Chickering/spark_example/spark-warehouse').
20/11/16 17:13:31 INFO SharedState: Warehouse path is 'file:/Users/grahamchickering/Desktop/Stat%20495/STAT495F20-project-Chickering/spark_example/spark-warehouse'.
20/11/16 17:13:34 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.7 using Spark classes.
20/11/16 17:13:34 INFO HiveConf: Found configuration file file:/Users/grahamchickering/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/16 17:13:35 INFO SessionState: Created HDFS directory: /tmp/hive/grahamchickering/90cfa19d-361e-41d4-8077-f9d21f60a9ba
20/11/16 17:13:35 INFO SessionState: Created local directory: /var/folders/q3/tpdx666x4cv2zhyblrxyhyl00000gn/T/grahamchickering/90cfa19d-361e-41d4-8077-f9d21f60a9ba
20/11/16 17:13:35 INFO SessionState: Created HDFS directory: /tmp/hive/grahamchickering/90cfa19d-361e-41d4-8077-f9d21f60a9ba/_tmp_space.db
20/11/16 17:13:35 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is file:/Users/grahamchickering/Desktop/Stat%20495/STAT495F20-project-Chickering/spark_example/spark-warehouse
20/11/16 17:13:35 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
20/11/16 17:13:35 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
20/11/16 17:13:35 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
20/11/16 17:13:35 INFO ObjectStore: ObjectStore, initialize called
20/11/16 17:13:35 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
20/11/16 17:13:35 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
20/11/16 17:13:36 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/11/16 17:13:38 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/11/16 17:13:38 INFO ObjectStore: Initialized ObjectStore
20/11/16 17:13:38 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
20/11/16 17:13:38 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore grahamchickering@10.112.183.126
20/11/16 17:13:38 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/11/16 17:13:38 INFO HiveMetaStore: Added admin role in metastore
20/11/16 17:13:38 INFO HiveMetaStore: Added public role in metastore
20/11/16 17:13:38 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/11/16 17:13:38 INFO HiveMetaStore: 0: get_all_functions
20/11/16 17:13:38 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_all_functions	
20/11/16 17:13:38 INFO HiveMetaStore: 0: get_database: default
20/11/16 17:13:38 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: default	
20/11/16 17:13:38 INFO HiveMetaStore: 0: get_database: global_temp
20/11/16 17:13:38 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: global_temp	
20/11/16 17:13:38 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
20/11/16 17:13:38 INFO HiveMetaStore: 0: get_database: default
20/11/16 17:13:38 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: default	
20/11/16 17:13:38 INFO HiveMetaStore: 0: get_database: default
20/11/16 17:13:38 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: default	
20/11/16 17:13:38 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/16 17:13:38 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/16 17:13:39 INFO CodeGenerator: Code generated in 206.018866 ms
20/11/16 17:13:39 INFO CodeGenerator: Code generated in 12.406193 ms
20/11/16 17:13:40 INFO SparkContext: Starting job: count at utils.scala:116
20/11/16 17:13:40 INFO DAGScheduler: Registering RDD 2 (count at utils.scala:116) as input to shuffle 0
20/11/16 17:13:40 INFO DAGScheduler: Got job 0 (count at utils.scala:116) with 1 output partitions
20/11/16 17:13:40 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:116)
20/11/16 17:13:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/11/16 17:13:40 INFO DAGScheduler: Missing parents: List()
20/11/16 17:13:40 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:116), which has no missing parents
20/11/16 17:13:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
20/11/16 17:13:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
20/11/16 17:13:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:58472 (size: 5.0 KiB, free: 912.3 MiB)
20/11/16 17:13:40 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1200
20/11/16 17:13:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/11/16 17:13:40 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/11/16 17:13:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
20/11/16 17:13:40 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)
20/11/16 17:13:40 INFO Executor: Fetching spark://localhost:58470/jars/sparklyr-3.0-2.12.jar with timestamp 1605564810529
20/11/16 17:13:40 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:58470 after 26 ms (0 ms spent in bootstraps)
20/11/16 17:13:40 INFO Utils: Fetching spark://localhost:58470/jars/sparklyr-3.0-2.12.jar to /private/var/folders/q3/tpdx666x4cv2zhyblrxyhyl00000gn/T/spark-020f9d68-f044-4322-a0a1-2f190c35412c/userFiles-ac715340-bb71-42b8-a61b-bab95e962517/fetchFileTemp8817205716388264465.tmp
20/11/16 17:13:40 INFO Executor: Adding file:/private/var/folders/q3/tpdx666x4cv2zhyblrxyhyl00000gn/T/spark-020f9d68-f044-4322-a0a1-2f190c35412c/userFiles-ac715340-bb71-42b8-a61b-bab95e962517/sparklyr-3.0-2.12.jar to class loader
20/11/16 17:13:40 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/11/16 17:13:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
20/11/16 17:13:40 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 2641 bytes result sent to driver
20/11/16 17:13:40 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 420 ms on localhost (executor driver) (1/1)
20/11/16 17:13:40 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/11/16 17:13:40 INFO DAGScheduler: ResultStage 1 (count at utils.scala:116) finished in 0.605 s
20/11/16 17:13:40 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/11/16 17:13:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
20/11/16 17:13:40 INFO DAGScheduler: Job 0 finished: count at utils.scala:116, took 0.649754 s
20/11/16 17:13:54 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:58472 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/11/16 17:13:54 INFO CodeGenerator: Code generated in 15.318531 ms
20/11/16 17:13:54 INFO CodeGenerator: Code generated in 24.435183 ms
20/11/16 17:13:54 INFO SparkContext: Starting job: collect at utils.scala:118
20/11/16 17:13:54 INFO DAGScheduler: Got job 1 (collect at utils.scala:118) with 2 output partitions
20/11/16 17:13:54 INFO DAGScheduler: Final stage: ResultStage 2 (collect at utils.scala:118)
20/11/16 17:13:54 INFO DAGScheduler: Parents of final stage: List()
20/11/16 17:13:54 INFO DAGScheduler: Missing parents: List()
20/11/16 17:13:54 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[17] at collect at utils.scala:118), which has no missing parents
20/11/16 17:13:54 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 51.2 KiB, free 912.3 MiB)
20/11/16 17:13:54 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 16.2 KiB, free 912.2 MiB)
20/11/16 17:13:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:58472 (size: 16.2 KiB, free: 912.3 MiB)
20/11/16 17:13:54 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
20/11/16 17:13:54 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[17] at collect at utils.scala:118) (first 15 tasks are for partitions Vector(0, 1))
20/11/16 17:13:54 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
20/11/16 17:13:54 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7393 bytes)
20/11/16 17:13:54 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 2, localhost, executor driver, partition 1, PROCESS_LOCAL, 7393 bytes)
20/11/16 17:13:54 INFO TaskSchedulerImpl: Registered BarrierCoordinator endpoint
20/11/16 17:13:54 INFO TaskSchedulerImpl: Successfully scheduled all the 2 tasks for barrier stage 2.
20/11/16 17:13:54 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
20/11/16 17:13:54 INFO Executor: Running task 1.0 in stage 2.0 (TID 2)
20/11/16 17:13:54 INFO CodeGenerator: Code generated in 11.881199 ms
20/11/16 17:13:54 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 1510 bytes result sent to driver
20/11/16 17:13:54 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 253 ms on localhost (executor driver) (1/2)
20/11/16 17:13:54 INFO CodeGenerator: Code generated in 30.360677 ms
20/11/16 17:13:55 ERROR Executor: Exception in task 1.0 in stage 2.0 (TID 2)
java.lang.Exception: sparklyr worker rscript failure with status 255, check worker logs for details.
	at sparklyr.Rscript.init(rscript.scala:83)
	at sparklyr.WorkerApply$$anon$2.run(workerapply.scala:133)
20/11/16 17:13:55 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.Exception: sparklyr worker rscript failure with status 255, check worker logs for details.
	at sparklyr.Rscript.init(rscript.scala:83)
	at sparklyr.WorkerApply$$anon$2.run(workerapply.scala:133)

20/11/16 17:13:55 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/11/16 17:13:55 INFO DAGScheduler: Marking ResultStage 2 (collect at utils.scala:118) as failed due to a barrier task failed.
20/11/16 17:13:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Task ResultTask(2, 1) from barrier stage ResultStage 2 (collect at utils.scala:118) failed.
20/11/16 17:13:55 INFO DAGScheduler: ResultStage 2 (collect at utils.scala:118) failed in 0.896 s due to Stage failed because barrier task ResultTask(2, 1) finished unsuccessfully.
java.lang.Exception: sparklyr worker rscript failure with status 255, check worker logs for details.
	at sparklyr.Rscript.init(rscript.scala:83)
	at sparklyr.WorkerApply$$anon$2.run(workerapply.scala:133)

20/11/16 17:13:55 INFO DAGScheduler: Job 1 failed: collect at utils.scala:118, took 0.906290 s
20/11/16 17:13:55 INFO DAGScheduler: Resubmitting ResultStage 2 (collect at utils.scala:118) due to barrier stage failure.
20/11/16 17:13:55 INFO DAGScheduler: Resubmitting failed stages
20/11/16 17:13:59 INFO SparkContext: Starting job: collect at utils.scala:118
20/11/16 17:13:59 INFO DAGScheduler: Got job 2 (collect at utils.scala:118) with 1 output partitions
20/11/16 17:13:59 INFO DAGScheduler: Final stage: ResultStage 3 (collect at utils.scala:118)
20/11/16 17:13:59 INFO DAGScheduler: Parents of final stage: List()
20/11/16 17:13:59 INFO DAGScheduler: Missing parents: List()
20/11/16 17:13:59 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[29] at collect at utils.scala:118), which has no missing parents
20/11/16 17:13:59 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 50.7 KiB, free 912.2 MiB)
20/11/16 17:13:59 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 16.1 KiB, free 912.2 MiB)
20/11/16 17:13:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:58472 (size: 16.1 KiB, free: 912.3 MiB)
20/11/16 17:13:59 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1200
20/11/16 17:13:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[29] at collect at utils.scala:118) (first 15 tasks are for partitions Vector(0))
20/11/16 17:13:59 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
20/11/16 17:13:59 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7393 bytes)
20/11/16 17:13:59 INFO TaskSchedulerImpl: Successfully scheduled all the 1 tasks for barrier stage 3.
20/11/16 17:13:59 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
20/11/16 17:14:00 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.lang.Exception: sparklyr worker rscript failure with status 255, check worker logs for details.
	at sparklyr.Rscript.init(rscript.scala:83)
	at sparklyr.WorkerApply$$anon$2.run(workerapply.scala:133)
20/11/16 17:14:00 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, localhost, executor driver): java.lang.Exception: sparklyr worker rscript failure with status 255, check worker logs for details.
	at sparklyr.Rscript.init(rscript.scala:83)
	at sparklyr.WorkerApply$$anon$2.run(workerapply.scala:133)

20/11/16 17:14:00 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/11/16 17:14:00 INFO DAGScheduler: Marking ResultStage 3 (collect at utils.scala:118) as failed due to a barrier task failed.
20/11/16 17:14:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Task ResultTask(3, 0) from barrier stage ResultStage 3 (collect at utils.scala:118) failed.
20/11/16 17:14:00 INFO DAGScheduler: ResultStage 3 (collect at utils.scala:118) failed in 0.622 s due to Stage failed because barrier task ResultTask(3, 0) finished unsuccessfully.
java.lang.Exception: sparklyr worker rscript failure with status 255, check worker logs for details.
	at sparklyr.Rscript.init(rscript.scala:83)
	at sparklyr.WorkerApply$$anon$2.run(workerapply.scala:133)

20/11/16 17:14:00 INFO DAGScheduler: Resubmitting ResultStage 3 (collect at utils.scala:118) due to barrier stage failure.
20/11/16 17:14:00 INFO DAGScheduler: Job 2 failed: collect at utils.scala:118, took 0.627229 s
20/11/16 17:14:00 INFO DAGScheduler: Resubmitting failed stages
20/11/16 17:14:06 INFO SparkContext: Starting job: collect at utils.scala:118
20/11/16 17:14:06 INFO DAGScheduler: Got job 3 (collect at utils.scala:118) with 2 output partitions
20/11/16 17:14:06 INFO DAGScheduler: Final stage: ResultStage 4 (collect at utils.scala:118)
20/11/16 17:14:06 INFO DAGScheduler: Parents of final stage: List()
20/11/16 17:14:06 INFO DAGScheduler: Missing parents: List()
20/11/16 17:14:06 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[41] at collect at utils.scala:118), which has no missing parents
20/11/16 17:14:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 51.3 KiB, free 912.1 MiB)
20/11/16 17:14:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 16.2 KiB, free 912.1 MiB)
20/11/16 17:14:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:58472 (size: 16.2 KiB, free: 912.3 MiB)
20/11/16 17:14:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1200
20/11/16 17:14:06 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[41] at collect at utils.scala:118) (first 15 tasks are for partitions Vector(0, 1))
20/11/16 17:14:06 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks
20/11/16 17:14:06 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 7393 bytes)
20/11/16 17:14:06 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 5, localhost, executor driver, partition 1, PROCESS_LOCAL, 7393 bytes)
20/11/16 17:14:06 INFO TaskSchedulerImpl: Successfully scheduled all the 2 tasks for barrier stage 4.
20/11/16 17:14:06 INFO Executor: Running task 1.0 in stage 4.0 (TID 5)
20/11/16 17:14:06 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
20/11/16 17:14:06 ERROR Executor: Exception in task 1.0 in stage 4.0 (TID 5)
java.lang.Exception: sparklyr worker rscript failure with status 255, check worker logs for details.
	at sparklyr.Rscript.init(rscript.scala:83)
	at sparklyr.WorkerApply$$anon$2.run(workerapply.scala:133)
20/11/16 17:14:06 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 4)
java.lang.Exception: sparklyr worker rscript failure with status 255, check worker logs for details.
	at sparklyr.Rscript.init(rscript.scala:83)
	at sparklyr.WorkerApply$$anon$2.run(workerapply.scala:133)
20/11/16 17:14:06 WARN TaskSetManager: Lost task 1.0 in stage 4.0 (TID 5, localhost, executor driver): java.lang.Exception: sparklyr worker rscript failure with status 255, check worker logs for details.
	at sparklyr.Rscript.init(rscript.scala:83)
	at sparklyr.WorkerApply$$anon$2.run(workerapply.scala:133)

20/11/16 17:14:06 INFO DAGScheduler: Marking ResultStage 4 (collect at utils.scala:118) as failed due to a barrier task failed.
20/11/16 17:14:06 INFO TaskSetManager: Lost task 0.0 in stage 4.0 (TID 4) on localhost, executor driver: java.lang.Exception (sparklyr worker rscript failure with status 255, check worker logs for details.) [duplicate 1]
20/11/16 17:14:06 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
20/11/16 17:14:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Task ResultTask(4, 1) from barrier stage ResultStage 4 (collect at utils.scala:118) failed.
20/11/16 17:14:06 INFO DAGScheduler: ResultStage 4 (collect at utils.scala:118) failed in 0.505 s due to Stage failed because barrier task ResultTask(4, 1) finished unsuccessfully.
java.lang.Exception: sparklyr worker rscript failure with status 255, check worker logs for details.
	at sparklyr.Rscript.init(rscript.scala:83)
	at sparklyr.WorkerApply$$anon$2.run(workerapply.scala:133)

20/11/16 17:14:06 INFO DAGScheduler: Resubmitting ResultStage 4 (collect at utils.scala:118) due to barrier stage failure.
20/11/16 17:14:06 INFO DAGScheduler: Job 3 failed: collect at utils.scala:118, took 0.509313 s
20/11/16 17:14:06 INFO DAGScheduler: Resubmitting failed stages
20/11/16 17:14:12 INFO SparkContext: Starting job: collect at utils.scala:118
20/11/16 17:14:12 INFO DAGScheduler: Got job 4 (collect at utils.scala:118) with 1 output partitions
20/11/16 17:14:12 INFO DAGScheduler: Final stage: ResultStage 5 (collect at utils.scala:118)
20/11/16 17:14:12 INFO DAGScheduler: Parents of final stage: List()
20/11/16 17:14:12 INFO DAGScheduler: Missing parents: List()
20/11/16 17:14:12 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[53] at collect at utils.scala:118), which has no missing parents
20/11/16 17:14:12 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 51.9 KiB, free 912.1 MiB)
20/11/16 17:14:12 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 912.0 MiB)
20/11/16 17:14:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:58472 (size: 16.3 KiB, free: 912.2 MiB)
20/11/16 17:14:12 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:58472 in memory (size: 16.2 KiB, free: 912.3 MiB)
20/11/16 17:14:12 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1200
20/11/16 17:14:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[53] at collect at utils.scala:118) (first 15 tasks are for partitions Vector(0))
20/11/16 17:14:12 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
20/11/16 17:14:12 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7393 bytes)
20/11/16 17:14:12 INFO TaskSchedulerImpl: Successfully scheduled all the 1 tasks for barrier stage 5.
20/11/16 17:14:12 INFO Executor: Running task 0.0 in stage 5.0 (TID 6)
20/11/16 17:14:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:58472 in memory (size: 16.2 KiB, free: 912.3 MiB)
20/11/16 17:14:12 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:58472 in memory (size: 16.1 KiB, free: 912.3 MiB)
20/11/16 17:14:12 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 6)
java.lang.Exception: sparklyr worker rscript failure with status 255, check worker logs for details.
	at sparklyr.Rscript.init(rscript.scala:83)
	at sparklyr.WorkerApply$$anon$2.run(workerapply.scala:133)
20/11/16 17:14:12 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 6, localhost, executor driver): java.lang.Exception: sparklyr worker rscript failure with status 255, check worker logs for details.
	at sparklyr.Rscript.init(rscript.scala:83)
	at sparklyr.WorkerApply$$anon$2.run(workerapply.scala:133)

20/11/16 17:14:12 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
20/11/16 17:14:12 INFO DAGScheduler: Marking ResultStage 5 (collect at utils.scala:118) as failed due to a barrier task failed.
20/11/16 17:14:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Task ResultTask(5, 0) from barrier stage ResultStage 5 (collect at utils.scala:118) failed.
20/11/16 17:14:12 INFO DAGScheduler: ResultStage 5 (collect at utils.scala:118) failed in 0.516 s due to Stage failed because barrier task ResultTask(5, 0) finished unsuccessfully.
java.lang.Exception: sparklyr worker rscript failure with status 255, check worker logs for details.
	at sparklyr.Rscript.init(rscript.scala:83)
	at sparklyr.WorkerApply$$anon$2.run(workerapply.scala:133)

20/11/16 17:14:12 INFO DAGScheduler: Resubmitting ResultStage 5 (collect at utils.scala:118) due to barrier stage failure.
20/11/16 17:14:12 INFO DAGScheduler: Job 4 failed: collect at utils.scala:118, took 0.520674 s
20/11/16 17:14:12 INFO DAGScheduler: Resubmitting failed stages
20/11/16 17:14:19 INFO SparkContext: Invoking stop() from shutdown hook
20/11/16 17:14:19 INFO SparkUI: Stopped Spark web UI at http://localhost:4040
20/11/16 17:14:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/11/16 17:14:19 INFO MemoryStore: MemoryStore cleared
20/11/16 17:14:19 INFO BlockManager: BlockManager stopped
20/11/16 17:14:19 INFO BlockManagerMaster: BlockManagerMaster stopped
20/11/16 17:14:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/11/16 17:14:19 INFO SparkContext: Successfully stopped SparkContext
20/11/16 17:14:19 INFO ShutdownHookManager: Shutdown hook called
20/11/16 17:14:19 INFO ShutdownHookManager: Deleting directory /private/var/folders/q3/tpdx666x4cv2zhyblrxyhyl00000gn/T/spark-020f9d68-f044-4322-a0a1-2f190c35412c
20/11/16 17:14:19 INFO ShutdownHookManager: Deleting directory /private/var/folders/q3/tpdx666x4cv2zhyblrxyhyl00000gn/T/spark-9461d6fa-fe31-48a2-8f4b-1680d1bb85e4
20/11/16 17:16:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/11/16 17:16:40 INFO SecurityManager: Changing view acls to: grahamchickering
20/11/16 17:16:40 INFO SecurityManager: Changing modify acls to: grahamchickering
20/11/16 17:16:40 INFO SecurityManager: Changing view acls groups to: 
20/11/16 17:16:40 INFO SecurityManager: Changing modify acls groups to: 
20/11/16 17:16:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(grahamchickering); groups with view permissions: Set(); users  with modify permissions: Set(grahamchickering); groups with modify permissions: Set()
20/11/16 17:16:41 INFO HiveConf: Found configuration file file:/Users/grahamchickering/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/16 17:16:42 INFO SparkContext: Running Spark version 3.0.0
20/11/16 17:16:42 INFO ResourceUtils: ==============================================================
20/11/16 17:16:42 INFO ResourceUtils: Resources for spark.driver:

20/11/16 17:16:42 INFO ResourceUtils: ==============================================================
20/11/16 17:16:42 INFO SparkContext: Submitted application: sparklyr
20/11/16 17:16:42 INFO SecurityManager: Changing view acls to: grahamchickering
20/11/16 17:16:42 INFO SecurityManager: Changing modify acls to: grahamchickering
20/11/16 17:16:42 INFO SecurityManager: Changing view acls groups to: 
20/11/16 17:16:42 INFO SecurityManager: Changing modify acls groups to: 
20/11/16 17:16:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(grahamchickering); groups with view permissions: Set(); users  with modify permissions: Set(grahamchickering); groups with modify permissions: Set()
20/11/16 17:16:42 INFO Utils: Successfully started service 'sparkDriver' on port 58925.
20/11/16 17:16:42 INFO SparkEnv: Registering MapOutputTracker
20/11/16 17:16:42 INFO SparkEnv: Registering BlockManagerMaster
20/11/16 17:16:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/11/16 17:16:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/11/16 17:16:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/11/16 17:16:42 INFO DiskBlockManager: Created local directory at /private/var/folders/q3/tpdx666x4cv2zhyblrxyhyl00000gn/T/blockmgr-1368489e-a392-4454-8019-c98677a0ea18
20/11/16 17:16:42 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
20/11/16 17:16:42 INFO SparkEnv: Registering OutputCommitCoordinator
20/11/16 17:16:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/11/16 17:16:42 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://localhost:4040
20/11/16 17:16:43 INFO SparkContext: Added JAR file:/Library/Frameworks/R.framework/Versions/4.0/Resources/library/sparklyr/java/sparklyr-3.0-2.12.jar at spark://localhost:58925/jars/sparklyr-3.0-2.12.jar with timestamp 1605565003033
20/11/16 17:16:43 INFO Executor: Starting executor ID driver on host localhost
20/11/16 17:16:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58926.
20/11/16 17:16:43 INFO NettyBlockTransferService: Server created on localhost:58926
20/11/16 17:16:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/11/16 17:16:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 58926, None)
20/11/16 17:16:43 INFO BlockManagerMasterEndpoint: Registering block manager localhost:58926 with 912.3 MiB RAM, BlockManagerId(driver, localhost, 58926, None)
20/11/16 17:16:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 58926, None)
20/11/16 17:16:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 58926, None)
20/11/16 17:16:43 INFO SharedState: loading hive config file: file:/Users/grahamchickering/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/16 17:16:43 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/grahamchickering/Desktop/Stat%20495/STAT495F20-project-Chickering/spark_example/spark-warehouse').
20/11/16 17:16:43 INFO SharedState: Warehouse path is 'file:/Users/grahamchickering/Desktop/Stat%20495/STAT495F20-project-Chickering/spark_example/spark-warehouse'.
20/11/16 17:16:46 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.7 using Spark classes.
20/11/16 17:16:46 INFO HiveConf: Found configuration file file:/Users/grahamchickering/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/16 17:16:47 INFO SessionState: Created HDFS directory: /tmp/hive/grahamchickering/93a2bbb8-84be-41d3-a8f7-fc308ccf4b36
20/11/16 17:16:47 INFO SessionState: Created local directory: /var/folders/q3/tpdx666x4cv2zhyblrxyhyl00000gn/T/grahamchickering/93a2bbb8-84be-41d3-a8f7-fc308ccf4b36
20/11/16 17:16:47 INFO SessionState: Created HDFS directory: /tmp/hive/grahamchickering/93a2bbb8-84be-41d3-a8f7-fc308ccf4b36/_tmp_space.db
20/11/16 17:16:47 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is file:/Users/grahamchickering/Desktop/Stat%20495/STAT495F20-project-Chickering/spark_example/spark-warehouse
20/11/16 17:16:47 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
20/11/16 17:16:47 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
20/11/16 17:16:47 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
20/11/16 17:16:47 INFO ObjectStore: ObjectStore, initialize called
20/11/16 17:16:47 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
20/11/16 17:16:47 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
20/11/16 17:16:49 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/11/16 17:16:50 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/11/16 17:16:50 INFO ObjectStore: Initialized ObjectStore
20/11/16 17:16:50 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
20/11/16 17:16:50 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore grahamchickering@10.112.183.126
20/11/16 17:16:50 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/11/16 17:16:50 INFO HiveMetaStore: Added admin role in metastore
20/11/16 17:16:50 INFO HiveMetaStore: Added public role in metastore
20/11/16 17:16:50 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/11/16 17:16:50 INFO HiveMetaStore: 0: get_all_functions
20/11/16 17:16:50 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_all_functions	
20/11/16 17:16:50 INFO HiveMetaStore: 0: get_database: default
20/11/16 17:16:50 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: default	
20/11/16 17:16:50 INFO HiveMetaStore: 0: get_database: global_temp
20/11/16 17:16:50 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: global_temp	
20/11/16 17:16:50 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
20/11/16 17:16:50 INFO HiveMetaStore: 0: get_database: default
20/11/16 17:16:50 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: default	
20/11/16 17:16:50 INFO HiveMetaStore: 0: get_database: default
20/11/16 17:16:50 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: default	
20/11/16 17:16:50 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/16 17:16:50 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/16 17:16:51 INFO CodeGenerator: Code generated in 165.123388 ms
20/11/16 17:16:51 INFO CodeGenerator: Code generated in 10.126079 ms
20/11/16 17:16:51 INFO SparkContext: Starting job: count at utils.scala:116
20/11/16 17:16:51 INFO DAGScheduler: Registering RDD 2 (count at utils.scala:116) as input to shuffle 0
20/11/16 17:16:51 INFO DAGScheduler: Got job 0 (count at utils.scala:116) with 1 output partitions
20/11/16 17:16:51 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:116)
20/11/16 17:16:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/11/16 17:16:51 INFO DAGScheduler: Missing parents: List()
20/11/16 17:16:51 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:116), which has no missing parents
20/11/16 17:16:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
20/11/16 17:16:52 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
20/11/16 17:16:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:58926 (size: 5.0 KiB, free: 912.3 MiB)
20/11/16 17:16:52 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1200
20/11/16 17:16:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:116) (first 15 tasks are for partitions Vector(0))
20/11/16 17:16:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/11/16 17:16:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
20/11/16 17:16:52 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)
20/11/16 17:16:52 INFO Executor: Fetching spark://localhost:58925/jars/sparklyr-3.0-2.12.jar with timestamp 1605565003033
20/11/16 17:16:52 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:58925 after 27 ms (0 ms spent in bootstraps)
20/11/16 17:16:52 INFO Utils: Fetching spark://localhost:58925/jars/sparklyr-3.0-2.12.jar to /private/var/folders/q3/tpdx666x4cv2zhyblrxyhyl00000gn/T/spark-029176cc-ee4b-4169-a785-a83d7eb2f071/userFiles-63a857c4-cce8-46ab-b04a-736e02a13f5e/fetchFileTemp8828981717812546846.tmp
20/11/16 17:16:52 INFO Executor: Adding file:/private/var/folders/q3/tpdx666x4cv2zhyblrxyhyl00000gn/T/spark-029176cc-ee4b-4169-a785-a83d7eb2f071/userFiles-63a857c4-cce8-46ab-b04a-736e02a13f5e/sparklyr-3.0-2.12.jar to class loader
20/11/16 17:16:52 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/11/16 17:16:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
20/11/16 17:16:52 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 2641 bytes result sent to driver
20/11/16 17:16:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 444 ms on localhost (executor driver) (1/1)
20/11/16 17:16:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/11/16 17:16:52 INFO DAGScheduler: ResultStage 1 (count at utils.scala:116) finished in 0.608 s
20/11/16 17:16:52 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/11/16 17:16:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
20/11/16 17:16:52 INFO DAGScheduler: Job 0 finished: count at utils.scala:116, took 0.651008 s
20/11/16 17:20:51 INFO HiveMetaStore: 0: get_database: default
20/11/16 17:20:51 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: default	
20/11/16 17:20:51 INFO HiveMetaStore: 0: get_database: default
20/11/16 17:20:51 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: default	
20/11/16 17:20:51 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/16 17:20:51 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/16 17:20:51 INFO SparkContext: Starting job: collect at utils.scala:43
20/11/16 17:20:51 INFO DAGScheduler: Job 1 finished: collect at utils.scala:43, took 0.000140 s
20/11/16 17:21:03 INFO HiveMetaStore: 0: get_database: default
20/11/16 17:21:03 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: default	
20/11/16 17:21:03 INFO HiveMetaStore: 0: get_database: default
20/11/16 17:21:03 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: default	
20/11/16 17:21:03 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/16 17:21:03 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/16 17:21:03 INFO SparkContext: Starting job: collect at utils.scala:43
20/11/16 17:21:03 INFO DAGScheduler: Job 2 finished: collect at utils.scala:43, took 0.000104 s
20/11/16 17:21:29 INFO HiveMetaStore: 0: get_database: default
20/11/16 17:21:29 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: default	
20/11/16 17:21:29 INFO HiveMetaStore: 0: get_database: default
20/11/16 17:21:29 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_database: default	
20/11/16 17:21:29 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/16 17:21:29 INFO audit: ugi=grahamchickering	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/16 17:21:29 INFO SparkContext: Starting job: collect at utils.scala:43
20/11/16 17:21:29 INFO DAGScheduler: Job 3 finished: collect at utils.scala:43, took 0.000112 s
20/11/16 17:23:02 INFO SparkContext: Invoking stop() from shutdown hook
20/11/16 17:23:02 INFO SparkUI: Stopped Spark web UI at http://localhost:4040
20/11/16 17:23:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/11/16 17:23:02 INFO MemoryStore: MemoryStore cleared
20/11/16 17:23:02 INFO BlockManager: BlockManager stopped
20/11/16 17:23:02 INFO BlockManagerMaster: BlockManagerMaster stopped
20/11/16 17:23:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/11/16 17:23:02 INFO SparkContext: Successfully stopped SparkContext
20/11/16 17:23:02 INFO ShutdownHookManager: Shutdown hook called
20/11/16 17:23:02 INFO ShutdownHookManager: Deleting directory /private/var/folders/q3/tpdx666x4cv2zhyblrxyhyl00000gn/T/spark-b8ce84bf-d0b3-4b6c-9e12-66be8bafe838
20/11/16 17:23:02 INFO ShutdownHookManager: Deleting directory /private/var/folders/q3/tpdx666x4cv2zhyblrxyhyl00000gn/T/spark-029176cc-ee4b-4169-a785-a83d7eb2f071
